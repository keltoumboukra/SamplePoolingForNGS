{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6df1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv \n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import PySimpleGUI as sg\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d37fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp \n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a96e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "input_file_folder = \"/Users/keltoumboukra/Desktop/Coding projects - Git/SamplePoolingForNGS/Files/ClariostarOutputs/Archive/\"\n",
    "input_file_name = \"qubit_data_Marko_22112022.CSV\"\n",
    "\n",
    "# final desired vol (nL)\n",
    "final_pool_vol = 10000\n",
    "\n",
    "# well position of pool in output plate\n",
    "pool_well_in_output_plate = \"B1\"\n",
    "\n",
    "# minimum volume to be pipetted (nL)\n",
    "min_pipetting_capacity = float(25) # for Echo\n",
    "\n",
    "# minimum volume that can be pipetted \n",
    "vol_available_in_well = 10000 # for e.g, if there is 20 uL in the well, 5000 nL is what can be available as 15 uL is the minimum working range in the PP-0200 Echo plate)\n",
    "\n",
    "# Beckman or Echo\n",
    "BeckmanOrEcho = \"Echo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f758c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.theme('LightBrown9')\n",
    "    \n",
    "layout = [\n",
    "        [sg.Text('Choose automated platform to use:\\n\\t- Echo: non genomic DNA\\n\\t- Beckman: genomic DNA\\n\\t- Beckman PacBio: Pooling for PacBio on Beckman')],\n",
    "        [sg.Text('Desired platform name:')],\n",
    "        [sg.DropDown([\"Echo\", \"Beckman\", \"Beckman PacBio\"], default_value=\"Echo\")],\n",
    "        [sg.Submit(), sg.Cancel()]\n",
    "        ]\n",
    "\n",
    "window = sg.Window('NGS Pooling', layout)\n",
    "event, values = window.read()\n",
    "window.maximize()\n",
    "window.close()\n",
    "\n",
    "BeckmanOrEcho = values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "647ecb9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/Users/keltoumboukra/Desktop/Coding projects - Git/SamplePoolingForNGS/Files/ClariostarOutputs/qubit_data_Marko_22112022.CSV' does not exist: b'/Users/keltoumboukra/Desktop/Coding projects - Git/SamplePoolingForNGS/Files/ClariostarOutputs/qubit_data_Marko_22112022.CSV'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/96/q5mqhh654rq681_cth8647w80000gp/T/ipykernel_9601/2378702689.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0minput_file_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0minput_file_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_file_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Concentration\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/Users/keltoumboukra/Desktop/Coding projects - Git/SamplePoolingForNGS/Files/ClariostarOutputs/qubit_data_Marko_22112022.CSV' does not exist: b'/Users/keltoumboukra/Desktop/Coding projects - Git/SamplePoolingForNGS/Files/ClariostarOutputs/qubit_data_Marko_22112022.CSV'"
     ]
    }
   ],
   "source": [
    "# Create GUI\n",
    "\n",
    "sg.theme('LightBrown9')\n",
    "\n",
    "\n",
    "if BeckmanOrEcho == \"Echo\":\n",
    "    layout = [\n",
    "        [sg.Text('Please enter the desired parameters')],\n",
    "        [sg.Text('Input file folder (ending with /)', size =(25, 1)), sg.InputText(\"/Users/keltoumboukra/Desktop/Coding projects - Git/SamplePoolingForNGS/Files/ClariostarOutputs/\")],\n",
    "        [sg.Text('Input file name', size =(25, 1)), sg.InputText(\"qubit_data_Marko_22112022.CSV\")],\n",
    "        [sg.Text('Output file folder (ending with /)', size =(25, 1)), sg.InputText(\"/Users/keltoumboukra/Desktop/Coding projects - Git/SamplePoolingForNGS/Files/\")],\n",
    "        [sg.Text('Final pool volume (nL)', size =(25, 1)), sg.InputText(\"1000\")],\n",
    "        [sg.Text('Pooling well position', size =(25, 1)), sg.InputText(\"B1\")],\n",
    "        [sg.Text('Minimum pipetting capacity (nL)', size =(25, 1)), sg.InputText(\"25\")],\n",
    "        [sg.Text('Sample volume available (nL)', size =(25, 1)), sg.InputText(\"5000\")],\n",
    "        [sg.Text('Exclude samples with concentration lower than (ng/uL)', size =(25, 1)), sg.InputText(\"0.35\")],\n",
    "        [sg.Submit(), sg.Cancel()]\n",
    "    ]\n",
    "\n",
    "elif BeckmanOrEcho == \"Beckman\" or \"Beckman PacBio\":\n",
    "    layout = [\n",
    "        [sg.Text('Please enter the desired parameters')],\n",
    "        [sg.Text('Input file folder (ending with /)', size =(25, 1)), sg.InputText(\"/Users/keltoumboukra/Desktop/Coding projects - Git/SamplePoolingForNGS/Files/ClariostarOutputs/\")],\n",
    "        [sg.Text('Input file name', size =(25, 1)), sg.InputText(\"qubit_data_Marko_22112022.CSV\")],\n",
    "        [sg.Text('Output file folder (ending with /)', size =(25, 1)), sg.InputText(\"/Users/keltoumboukra/Desktop/Coding projects - Git/SamplePoolingForNGS/Files/\")],\n",
    "        [sg.Text('Final pool volume (uL)', size =(25, 1)), sg.InputText(\"10\")],\n",
    "        [sg.Text('Pooling well position', size =(25, 1)), sg.InputText(\"B1\")],\n",
    "        [sg.Text('Minimum pipetting capacity (uL)', size =(25, 1)), sg.InputText(\"0.5\")],\n",
    "        [sg.Text('Sample volume available (uL)', size =(25, 1)), sg.InputText(\"25\")],\n",
    "        [sg.Text('Exclude conc (ng/uL) lower than', size =(25, 1)), sg.InputText(\"0.35\")],\n",
    "        [sg.Submit(), sg.Cancel()]\n",
    "    ]     \n",
    "\n",
    "\n",
    "window = sg.Window('Robot File Generator For NGS Pooling', layout)\n",
    "event, values = window.read()\n",
    "window.maximize()\n",
    "window.close()    \n",
    "\n",
    "# Assigning values \n",
    "input_file_folder = values[0]\n",
    "input_file_name = values[1]\n",
    "output_files_folder = values[2]\n",
    "final_pool_vol = float(values[3])\n",
    "pool_well_in_output_plate = values[4]\n",
    "min_pipetting_capacity = float(values[5])\n",
    "vol_available_in_well = float(values[6])\n",
    "min_threshold_values = float(values[7])\n",
    "\n",
    "# Dataframes\n",
    "input_file_df = pd.read_csv(input_file_folder + input_file_name)\n",
    "input_file_df.rename(columns={input_file_df.columns[2]: \"Concentration\"},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "855698a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/keltoumboukra/Desktop/Coding projects - Git/SamplePoolingForNGS/Files/NGSpooling_20230321-153219/InputFiles/InputFile.csv'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outputs\n",
    "\n",
    "output_files_folder = output_files_folder + \"NGSpooling_\" + timestr\n",
    "os.mkdir(output_files_folder)\n",
    "output_files_folder = output_files_folder + \"/\"\n",
    "\n",
    "output_file_name = \"ProcessingDetails.csv\"\n",
    "output_file_df = pd.DataFrame(columns=['Sample Well ID','Sample Concentration','Sample Calculated Weight', 'Sample Calculated Weight Normalised', 'Sample Calculated Volume In Pool'])\n",
    "\n",
    "if BeckmanOrEcho == \"Echo\":\n",
    "    output_echo_file_name = \"EchoFile.csv\"\n",
    "    output_echo_file_df = pd.DataFrame(columns=['Source Well','Destination Well','Transfer Volume'])\n",
    "elif BeckmanOrEcho == \"Beckman\":\n",
    "    output_beckman_file_name = \"BeckmanFile.csv\"\n",
    "    output_beckman_file_df = pd.DataFrame(columns=['Source Well','Destination Well','Transfer Volume'])    \n",
    "elif BeckmanOrEcho == \"Beckman PacBio\":\n",
    "    output_beckmanPacBio_file_name = \"BeckmanPacBioFile.csv\"\n",
    "    output_beckmanPacBio_file_df = pd.DataFrame(columns=['Source Well 384','Source Well 96','Source Plate 96','Destination Well','Transfer Volume']) \n",
    "    \n",
    "output_report_name = \"ExceptionsReport.csv\"\n",
    "output_report_df = pd.DataFrame(columns=['Sample Well ID','Sample Concentration','Comment'])\n",
    "\n",
    "# Copy input file into the output folder\n",
    "os.mkdir(output_files_folder + \"InputFiles/\")\n",
    "shutil.copyfile(input_file_folder+input_file_name, output_files_folder + \"InputFiles/\" + \"InputFile.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7eb00515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim input data frame \n",
    "\n",
    "# Remove standards \n",
    "input_file_df = input_file_df[input_file_df[\"Content\"].str.contains(\"Standard\") == False]\n",
    "input_file_df = input_file_df.reset_index(drop=True)\n",
    "\n",
    "# Remove samples with negative concentrations and add to output report\n",
    "for row in input_file_df.itertuples():\n",
    "    \n",
    "    Index = row [0]\n",
    "    Well = row[1]\n",
    "    Concentration = row[3]\n",
    "    \n",
    "    if Concentration <= min_threshold_values:\n",
    "        output_report_df = output_report_df.append({'Sample Well ID': Well, 'Sample Concentration': Concentration, 'Comment': \"Sample has negative value for concentration\"}, ignore_index=True)\n",
    "        input_file_df = input_file_df.drop(Index)\n",
    "\n",
    "input_file_df = input_file_df.reset_index(drop=True)\n",
    "output_report_df = output_report_df.reset_index(drop=True)\n",
    "\n",
    "# Copy trimmed input file into the output folder\n",
    "input_file_df.to_csv(output_files_folder + \"InputFiles/\" + \"InputTrimmedFile.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d0c95a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Weights \n",
    "\n",
    "max_concentration = input_file_df['Concentration'].max()\n",
    "sum_weights = float()\n",
    "\n",
    "for row in input_file_df.itertuples():\n",
    "    \n",
    "    Index = row [0]\n",
    "    Well = row[1]\n",
    "    Concentration = row[3]\n",
    "\n",
    "    sample_weight = max_concentration/Concentration\n",
    "    sum_weights += sample_weight\n",
    "    output_file_df = output_file_df.append({'Sample Well ID': Well, 'Sample Concentration': Concentration, 'Sample Calculated Weight': sample_weight}, ignore_index=True)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ece0ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Normalised Weights \n",
    "\n",
    "sum_normalised_weights = float(0)\n",
    "for row in input_file_df.itertuples():\n",
    "    \n",
    "    Index = row [0]\n",
    "    Well = row[1]\n",
    "    Concentration = row[3]\n",
    "    \n",
    "    sample_normalised_weight = output_file_df[\"Sample Calculated Weight\"].iloc[Index]/sum_weights\n",
    "    sum_normalised_weights += sample_normalised_weight # for QC, should be =1\n",
    "    output_file_df.at[Index,'Sample Calculated Weight Normalised']=sample_normalised_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55d08cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plate': 2, 'well': 'D5'}\n",
      "2\n",
      "D5\n"
     ]
    }
   ],
   "source": [
    "# Translate 384 coordinates to 96 coordinates for Beckman PacBio\n",
    "\n",
    "def _translate(id):\n",
    "  id = id -1\n",
    "  if (id // 24) % 2 ==0:\n",
    "    if id % 2==0:\n",
    "        destinationrow = 1 + id % 24 //2\n",
    "        destinationCol = chr(ord('@')+1+id // 48)\n",
    "        return { \"plate\": 1, \"well\": destinationCol + str(destinationrow)}\n",
    "    else:\n",
    "        destinationrow = 1 + id % 24 //2\n",
    "        destinationCol = chr(ord('@')+1+id // 48)\n",
    "        return { \"plate\": 2, \"well\": destinationCol + str(destinationrow)}\n",
    "  else:\n",
    "    if id % 2==0:\n",
    "        destinationrow =  1 +id % 24 //2\n",
    "        destinationCol = chr(ord('@')+1+id // 48)\n",
    "        return { \"plate\": 3, \"well\": destinationCol + str(destinationrow)}\n",
    "    else:\n",
    "        destinationrow =  1 +id % 24 //2\n",
    "        destinationCol = chr(ord('@')+1+id // 48)\n",
    "        return { \"plate\": 4, \"well\": destinationCol + str(destinationrow)}\n",
    "\n",
    "# Generate dictionary to to convert 384 well number to 96-well plate number and well coordinate\n",
    "\n",
    "x = list(map(_translate, [i+1 for i in range(0,384)]))\n",
    "\n",
    "rows = [i for i in string.ascii_uppercase[:16]]\n",
    "columns = [i for i in range(1, 24 + 1)]\n",
    "wells = [letter + str(number) if number >= 10 else letter + \"0\" + str(number) for letter in rows for number in columns]\n",
    "\n",
    "\n",
    "d1=dict(zip(wells,x))\n",
    "d1\n",
    "\n",
    "#print(d1[\"G10\"])\n",
    "#print(d1[\"G10\"][\"plate\"])\n",
    "#print(d1[\"G10\"][\"well\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3ba5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Volume to pipette for each sample in the final pool\n",
    "\n",
    "vol_in_pool = float()\n",
    "sum_vol_in_pool = float(0)\n",
    "fail_bool = bool(0)\n",
    "\n",
    "for row in input_file_df.itertuples():\n",
    "    \n",
    "    Index = row [0]\n",
    "    Well = row[1]\n",
    "    Concentration = row[3]\n",
    "    \n",
    "    vol_in_pool = output_file_df[\"Sample Calculated Weight Normalised\"].iloc[Index]*final_pool_vol \n",
    "    sum_vol_in_pool += vol_in_pool # for QC, should be = final_pool_vol  \n",
    "    \n",
    "    if vol_in_pool < min_pipetting_capacity:\n",
    "        output_report_df = output_report_df.append({'Sample Well ID': Well, 'Sample Concentration': Concentration, 'Comment': \"Volume to be pipetted for this well ({} nL) is smaller than pipetting capacity ({} nL). You MUST reprocess the file\".format(vol_in_pool,min_pipetting_capacity)}, ignore_index=True)\n",
    "        fail_bool = bool(1)\n",
    "    elif vol_in_pool > vol_available_in_well:\n",
    "        output_report_df = output_report_df.append({'Sample Well ID': Well, 'Sample Concentration': Concentration, 'Comment': \"Volume to be pipetted for this well ({} nL) is larger than the volume available ({} nL). You MUST reprocess the file\".format(vol_in_pool,vol_available_in_well)}, ignore_index=True)\n",
    "        fail_bool = bool(1)\n",
    "    else:\n",
    "        output_file_df.at[Index,'Sample Calculated Volume In Pool']=vol_in_pool\n",
    "        if BeckmanOrEcho == \"Echo\":\n",
    "            output_echo_file_df = output_echo_file_df.append({'Source Well': Well, 'Destination Well': pool_well_in_output_plate, 'Transfer Volume': vol_in_pool}, ignore_index=True)\n",
    "        elif BeckmanOrEcho == \"Beckman\":\n",
    "            output_beckman_file_df = output_beckman_file_df.append({'Source Well': Well, 'Destination Well': pool_well_in_output_plate, 'Transfer Volume': vol_in_pool}, ignore_index=True)\n",
    "        elif BeckmanOrEcho == \"Beckman PacBio\":\n",
    "            output_beckmanPacBio_file_df = output_beckmanPacBio_file_df.append({'Source Well 384': Well,'Source Well 96': d1[Well][\"well\"],'Source Plate 96' : d1[Well][\"plate\"],'Destination Well' : pool_well_in_output_plate,'Transfer Volume': vol_in_pool}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d74c3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final pool concentration for QC \n",
    "\n",
    "sum_concentrations = float(0)\n",
    "\n",
    "for row in output_file_df.itertuples():\n",
    "    \n",
    "    Concentration = row[2]\n",
    "    VolInPool = row[5]\n",
    "    \n",
    "    sum_concentrations += (Concentration*VolInPool)\n",
    "\n",
    "final_pool_concentration = \"%.3f\"%(sum_concentrations / final_pool_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0512a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export files and communicate outcome to user in GUI\n",
    "\n",
    "os.mkdir(output_files_folder + \"OutputFiles/\")\n",
    "\n",
    "if BeckmanOrEcho == \"Echo\":\n",
    "    if fail_bool == bool(1):\n",
    "        layout = [[sg.Text(\"Processing failed. The concentration of 1 sample or more is out of range and doesn't allow pooling with the settings entered. Check report file for details.\")], [sg.Button(\"OK\")]]\n",
    "        window = sg.Window(\"Echo File Generator For NGS Pooling\", layout)\n",
    "    else:\n",
    "        output_file_df.to_csv(output_files_folder + \"OutputFiles/\" + output_file_name, index=False)\n",
    "        output_echo_file_df.to_csv(output_files_folder + \"OutputFiles/\" + output_echo_file_name, index=False)\n",
    "        layout = [[sg.Text(\"Processing successful! Final pool concentration: {} ng/uL.\".format(final_pool_concentration))], [sg.Button(\"OK\")]]\n",
    "        window = sg.Window(\"Echo File Generator For NGS Pooling\", layout)\n",
    "\n",
    "elif BeckmanOrEcho == \"Beckman\":\n",
    "    if fail_bool == bool(1):\n",
    "        layout = [[sg.Text(\"Processing failed. The concentration of 1 sample or more is out of range and doesn't allow pooling with the settings entered. Check report file for details.\")], [sg.Button(\"OK\")]]\n",
    "        window = sg.Window(\"Beckman File Generator For NGS Pooling\", layout)\n",
    "    else:\n",
    "        output_file_df.to_csv(output_files_folder + \"OutputFiles/\" + output_file_name, index=False)\n",
    "        output_beckman_file_df = output_beckman_file_df.sort_values(by='Transfer Volume', ascending=False)\n",
    "        output_beckman_file_df.to_csv(output_files_folder + \"OutputFiles/\" + output_beckman_file_name, index=False)\n",
    "        layout = [[sg.Text(\"Processing successful! Final pool concentration: {} ng/uL.\".format(final_pool_concentration))], [sg.Button(\"OK\")]]\n",
    "        window = sg.Window(\"Beckman File Generator For NGS Pooling\", layout)\n",
    "        \n",
    "elif BeckmanOrEcho == \"Beckman PacBio\":\n",
    "    if fail_bool == bool(1):\n",
    "        layout = [[sg.Text(\"Processing failed. The concentration of 1 sample or more is out of range and doesn't allow pooling with the settings entered. Check report file for details.\")], [sg.Button(\"OK\")]]\n",
    "        window = sg.Window(\"Beckman File Generator For NGS Pooling\", layout)\n",
    "    else:\n",
    "        output_file_df.to_csv(output_files_folder + \"OutputFiles/\" + output_file_name, index=False)\n",
    "        output_beckmanPacBio_file_df = output_beckmanPacBio_file_df.sort_values(by='Transfer Volume', ascending=False)\n",
    "        output_beckmanPacBio_file_df.to_csv(output_files_folder + \"OutputFiles/\" + output_beckmanPacBio_file_name, index=False)\n",
    "        layout = [[sg.Text(\"Processing successful! Final pool concentration: {} ng/uL.\".format(final_pool_concentration))], [sg.Button(\"OK\")]]\n",
    "        window = sg.Window(\"Beckman PacBio File Generator For NGS Pooling\", layout)\n",
    "\n",
    "output_report_df.to_csv(output_files_folder + \"OutputFiles/\" + output_report_name, index=False)\n",
    "\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "    if event == \"OK\" or event == sg.WIN_CLOSED:\n",
    "        break\n",
    "window.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e0a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb3b92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
